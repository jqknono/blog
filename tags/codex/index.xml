<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Codex on jqknono Blogs</title><link>https://blog.jqknono.com/tags/codex/</link><description>Recent content in Codex on jqknono Blogs</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><managingEditor>https://blog.jqknono.com (jqknono)</managingEditor><webMaster>https://blog.jqknono.com (jqknono)</webMaster><lastBuildDate>Tue, 17 Feb 2026 10:30:00 +0800</lastBuildDate><atom:link href="https://blog.jqknono.com/tags/codex/index.xml" rel="self" type="application/rss+xml"/><item><title>GPT-5.3-Codex First Impressions: From Surprise to Rational Assessment</title><link>https://blog.jqknono.com/blog/2026/02/17/gpt-53-codex-experience/</link><pubDate>Tue, 17 Feb 2026 10:30:00 +0800</pubDate><author>https://blog.jqknono.com (jqknono)</author><guid>https://blog.jqknono.com/blog/2026/02/17/gpt-53-codex-experience/</guid><description>&lt;p&gt;OpenAI, before the official release of GPT-5.3, has rolled out the specialized model GPT-5.3-Codex. From a business perspective, this decision is easy to understand. GPT-5.3-Codex is priced the same as the standard GPT-5.3, but its outputs are more proactive, execution time is shorter, and memory usage is lower, which translates to higher profit margins. For OpenAI, GPT-5.3-Codex is clearly a more cost-effective option.&lt;/p&gt;
&lt;p&gt;During the first week after GPT-5.3-Codex was released, the user experience was indeed impressive. The model&amp;rsquo;s response speed was noticeably better than previous versions, and code generation feedback was very prompt. In development scenarios that require rapid iteration and frequent interaction, this efficiency boost brings tangible productivity improvements. When multiple implementation options or quick idea validation are needed in a short time, Codex&amp;rsquo;s proactive output proves especially useful.&lt;/p&gt;</description></item><item><title>Cost Formulas and Break-Even Points for Vibe Coding</title><link>https://blog.jqknono.com/blog/2026/01/07/vibe-coding-cost-formulas/</link><pubDate>Wed, 07 Jan 2026 10:00:00 +0800</pubDate><author>https://blog.jqknono.com (jqknono)</author><guid>https://blog.jqknono.com/blog/2026/01/07/vibe-coding-cost-formulas/</guid><description>&lt;p&gt;AI coding tools can be categorized into three billing models:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Per-token billing&lt;/strong&gt;: Includes various APIs, Claude Code (Claude Pro), Codex Cli (ChatGPT Plus), Zhipu Lite/Pro, Cursor (new version), etc. These are fundamentally token-based, with some products offering package discounts.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Per-API call billing&lt;/strong&gt;: Such as OpenRouter (free quota), ModelScope, Gemini Code Assistant (1,000 free calls per day), Chutes, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Per-prompt billing&lt;/strong&gt;: Such as Cursor (legacy version, 500 prompts), Github Copilot (300 prompts), etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These three models are essentially paying for model inference and context processing, differing only in pricing granularity and quota forms.&lt;/p&gt;</description></item></channel></rss>