<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on jqknono Blogs</title><link>https://blog.jqknono.com/zh-cn/categories/ai/</link><description>Recent content in AI on jqknono Blogs</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Fri, 12 Dec 2025 11:46:01 +0800</lastBuildDate><atom:link href="https://blog.jqknono.com/zh-cn/categories/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>OpenAI的取名艺术鉴赏</title><link>https://blog.jqknono.com/zh-cn/blog/2025/12/12/openai%E7%9A%84%E5%8F%96%E5%90%8D%E8%89%BA%E6%9C%AF%E9%89%B4%E8%B5%8F/</link><pubDate>Fri, 12 Dec 2025 11:46:01 +0800</pubDate><guid>https://blog.jqknono.com/zh-cn/blog/2025/12/12/openai%E7%9A%84%E5%8F%96%E5%90%8D%E8%89%BA%E6%9C%AF%E9%89%B4%E8%B5%8F/</guid><description>&lt;p&gt;这里&lt;a href="https://platform.openai.com/docs/models/"&gt;https://platform.openai.com/docs/models/&lt;/a&gt;记录了 OpenAI 的所有模型.&lt;/p&gt;
&lt;p&gt;太远了的不说, 从 &lt;code&gt;GPT-4&lt;/code&gt; 系列及同世代模型聊起, 这是我汇总的表格:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;th&gt;模型卡片链接&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ChatGPT-4o&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;GPT-4o model used in ChatGPT&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/chatgpt-4o-latest"&gt;https://platform.openai.com/docs/models/chatgpt-4o-latest&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;An older high-intelligence GPT model&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4"&gt;https://platform.openai.com/docs/models/gpt-4&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4 Turbo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;An older high-intelligence GPT model&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4-turbo"&gt;https://platform.openai.com/docs/models/gpt-4-turbo&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4 Turbo Preview&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deprecated An older fast GPT model&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4-turbo-preview"&gt;https://platform.openai.com/docs/models/gpt-4-turbo-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4.1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Smartest non-reasoning model&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4.1"&gt;https://platform.openai.com/docs/models/gpt-4.1&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4.1 mini&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Smaller, faster version of GPT-4.1&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4.1-mini"&gt;https://platform.openai.com/docs/models/gpt-4.1-mini&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4.1 nano&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fastest, most cost-efficient version of GPT-4.1&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4.1-nano"&gt;https://platform.openai.com/docs/models/gpt-4.1-nano&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4.5 Preview&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deprecated large model.&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4.5-preview"&gt;https://platform.openai.com/docs/models/gpt-4.5-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fast, intelligent, flexible GPT model&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o"&gt;https://platform.openai.com/docs/models/gpt-4o&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o Audio&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;GPT-4o models capable of audio inputs and outputs&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-audio-preview"&gt;https://platform.openai.com/docs/models/gpt-4o-audio-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o mini&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fast, affordable small model for focused tasks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-mini"&gt;https://platform.openai.com/docs/models/gpt-4o-mini&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o mini Audio&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Smaller model capable of audio inputs and outputs&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-mini-audio-preview"&gt;https://platform.openai.com/docs/models/gpt-4o-mini-audio-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o mini Realtime&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Smaller realtime model for text and audio inputs and outputs&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-mini-realtime-preview"&gt;https://platform.openai.com/docs/models/gpt-4o-mini-realtime-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o mini Search Preview&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fast, affordable small model for web search&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-mini-search-preview"&gt;https://platform.openai.com/docs/models/gpt-4o-mini-search-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o mini Transcribe&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Speech-to-text model powered by GPT-4o mini&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-mini-transcribe"&gt;https://platform.openai.com/docs/models/gpt-4o-mini-transcribe&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o mini TTS&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Text-to-speech model powered by GPT-4o mini&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-mini-tts"&gt;https://platform.openai.com/docs/models/gpt-4o-mini-tts&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o Realtime&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Model capable of realtime text and audio inputs and outputs&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-realtime-preview"&gt;https://platform.openai.com/docs/models/gpt-4o-realtime-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o Search Preview&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;GPT model for web search in Chat Completions&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-search-preview"&gt;https://platform.openai.com/docs/models/gpt-4o-search-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o Transcribe&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Speech-to-text model powered by GPT-4o&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-transcribe"&gt;https://platform.openai.com/docs/models/gpt-4o-transcribe&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o Transcribe Diarize&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Transcription model that identifies who&amp;rsquo;s speaking when&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-transcribe-diarize"&gt;https://platform.openai.com/docs/models/gpt-4o-transcribe-diarize&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Previous full o-series reasoning model&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o1"&gt;https://platform.openai.com/docs/models/o1&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o1-mini&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A small model alternative to o1&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o1-mini"&gt;https://platform.openai.com/docs/models/o1-mini&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o1-pro&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Version of o1 with more compute for better responses&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o1-pro"&gt;https://platform.openai.com/docs/models/o1-pro&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Reasoning model for complex tasks, succeeded by GPT-5&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o3"&gt;https://platform.openai.com/docs/models/o3&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o3-pro&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Version of o3 with more compute for better responses&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o3-pro"&gt;https://platform.openai.com/docs/models/o3-pro&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o3-mini&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A small model alternative to o3&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o3-mini"&gt;https://platform.openai.com/docs/models/o3-mini&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o4-mini&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fast, cost-efficient reasoning model, succeeded by GPT-5 mini&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o4-mini"&gt;https://platform.openai.com/docs/models/o4-mini&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;针对音频(Audio), 实时(Realtime), 搜索(Search), 音频到文字(Transcribe), 文字到声音(TTS) 等场景, 全都有相应模型.&lt;br&gt;
对同一场景, 如 音频&lt;code&gt;Audio&lt;/code&gt; 场景, 提供了 &lt;code&gt;GPT-4o Audio&lt;/code&gt; 和 &lt;code&gt;GPT-4o mini Audio&lt;/code&gt; 两个模型, 用户需要通过尝试决定能否接受效果.&lt;br&gt;
音频到文字&lt;code&gt;Transcribe&lt;/code&gt; 场景, 提供了 &lt;code&gt;GPT-4o Transcribe&lt;/code&gt;, &lt;code&gt;GPT-4o mini Transcribe&lt;/code&gt;, &lt;code&gt;GPT-4o Transcribe Diarize&lt;/code&gt; 三个模型.&lt;br&gt;
&lt;code&gt;ChatGPT-4o&lt;/code&gt; 是 &lt;code&gt;GPT-4o&lt;/code&gt; 的特别版, 用于 ChatGPT 中, 其他场景不能用.
&lt;code&gt;GPT-4.1&lt;/code&gt;比&lt;code&gt;GPT-4&lt;/code&gt;更好, 是符合直觉的, 但&lt;code&gt;GPT-4o&lt;/code&gt;(4 欧)和&lt;code&gt;GPT-4.1&lt;/code&gt;或&lt;code&gt;GPT-4&lt;/code&gt;却不好比较. 时间线上, 先出现的是&lt;code&gt;GPT-4&lt;/code&gt;, 然后是&lt;code&gt;GPT-4o&lt;/code&gt;, 再然后是&lt;code&gt;GPT-4.1&lt;/code&gt;, 当然, 仍然无法判断&lt;code&gt;GPT-4o&lt;/code&gt;和&lt;code&gt;GPT-4.1&lt;/code&gt;谁更好.
这里插入下对&amp;quot;好&amp;quot;的定义, 我个人将数学和推理结果正确率高作为&amp;quot;好&amp;quot;的定义, 完全不将&amp;quot;速度&amp;quot;纳入&amp;quot;好&amp;quot;的定义. 但此前 OpenAI 可能认为速度和智慧同样重要, 因此出现奇怪的模型对比. 进入 GPT-5 时代后, 庆幸 OpenAI 开始放弃速度, 追求智慧, 至此模型间的对比才没有歧义. 一个快速的错误回答是浪费时间, 没有意义.&lt;/p&gt;</description></item><item><title>gpt-5-high是最适合开发者的模型</title><link>https://blog.jqknono.com/zh-cn/blog/2025/11/26/gpt-5-high%E6%98%AF%E6%9C%80%E9%80%82%E5%90%88%E5%BC%80%E5%8F%91%E8%80%85%E7%9A%84%E6%A8%A1%E5%9E%8B/</link><pubDate>Wed, 26 Nov 2025 14:44:57 +0800</pubDate><guid>https://blog.jqknono.com/zh-cn/blog/2025/11/26/gpt-5-high%E6%98%AF%E6%9C%80%E9%80%82%E5%90%88%E5%BC%80%E5%8F%91%E8%80%85%E7%9A%84%E6%A8%A1%E5%9E%8B/</guid><description>&lt;p&gt;如果是需要编码的话，gpt-5-high 是目前唯一能真正提效的模型。&lt;/p&gt;
&lt;p&gt;我有 10 个月的 claude 模型使用经验，Gemini/DeepSeek/glm/grok 零星使用，非常讨厌那些不带思考的模型。必须承认 claude 能长时间的工作，擅长工具使用，但是结果错误率很高，导致成果可用率低，经常需要调整，但 claude 的调整能力很差，会反复的，大量的，颠覆性的，画蛇添足的，自由散漫的遨游代码库，并随地拉屎。我在出现过多次数小时的工作不得 hard reset 之后，对 claude 这种勤快的笨鸟行为深感厌恶。它的工作风格很适合做调研，得到一个看上去有点道理，但经不起推敲的水文。或者操作浏览器，执行工具，写点脚本，修改少量页面，上限就在这儿。&lt;/p&gt;
&lt;p&gt;这里不讨论提示词的用法，如果有用得好 claude 的兄弟，继续用就好，我可能和 claude 相性不符，合作不来。&lt;/p&gt;
&lt;p&gt;然后我推荐的编码模型仅有一款，就是 gpt-5-high，仅此一款。gpt-5 的 nano，mini，medium，gpt-codex，gpt-codex-high，gpt-codex-max 等，都不在推荐之列，它们和 gpt5high 完全不是一个东西。所有仅展示模型是“gpt-5”的都不是 gpt-5-high，多了字符少了字符都不是“gpt-5-high”。&lt;/p&gt;
&lt;p&gt;和 gpt-5-high 行为最相似的是 OpenAI 的 o3 模型，要是没有 gpt5high 使用渠道的能用几次 o3 也可以感受到模型智力。点名 vscode github copilot 里，曾经有过 o3，但由于 vscode 的拉胯，它在 vscode 里仅能用于 ask，并且单次会话消耗 5 次高级请求，我在长期的 copilot 订阅里从未用过 o3，是转到 cursor 以后才发现 o3 智力水平这么高。vscode github copilot 已经下架 o3，并声称 gpt5.1 可作替代品。负责任的说，有 high 没 high 根本不是一个东西，非常建议直接弃用 copilot。如果是写点小东西的学生，预算有限，copilot 也可以带入个门。&lt;/p&gt;</description></item><item><title>llm的伪人感</title><link>https://blog.jqknono.com/zh-cn/blog/2025/11/24/llm%E7%9A%84%E4%BC%AA%E4%BA%BA%E6%84%9F/</link><pubDate>Mon, 24 Nov 2025 16:03:46 +0800</pubDate><guid>https://blog.jqknono.com/zh-cn/blog/2025/11/24/llm%E7%9A%84%E4%BC%AA%E4%BA%BA%E6%84%9F/</guid><description>&lt;p&gt;一些论坛会抵制 AI 模型假装人类参与论坛活动, 如发帖回帖等, 继而大家开始&amp;quot;猎巫&amp;quot;, 碰到看起来表达怪异的帖子, 会判断其是否是 AI 生成的内容, 继而展开一些讨论.&lt;/p&gt;
&lt;p&gt;为何 AI 生成内容会被识别? 猜测可能 AI 生成的东西有一种&amp;quot;伪人感&amp;quot;. 尽管 AI 被投喂互联网海量人类活动数据, 但 AI 仍然常常给人违和感, 可能它没有身体的触感神经, 没有内分泌激素, 可能它不向往社会连接, 欲望与人类的欲望相差甚远. AI 与人类的对话中, 没有&amp;quot;拐弯抹角的炫耀自己、添油加醋的贬低别人、相互窥探的搬弄是非&amp;quot;, AI 不炫耀自己, 不贬低第三方, 对提问者似乎也不感兴趣, 感觉它像一个和尚, 几乎没有情绪, 只是解决问题.&lt;/p&gt;
&lt;p&gt;尽管人类自己经常犯错, 但人类向往&amp;quot;正确&amp;quot;, 希望 AI 给出正确的产物. 是否是这种对&amp;quot;正确&amp;quot;的追求造成了 AI 的伪人感? AI 也较少给人&amp;quot;自我怀疑&amp;quot;感, 即使是非常愚蠢的小模型, 也能自信满满, 夸夸其谈. 一些较傻的 AI 模型对其知识库深信不疑, 它们可能有着错误的元认知, 缺少怀疑精神, 不过这也不应该给人&amp;quot;伪人&amp;quot;感, 傻逼和&amp;quot;伪人&amp;quot;并不一样.&lt;/p&gt;
&lt;p&gt;AI 是否有价值观倾向? 网页端模型服务的输出通常会加一道门禁, 避免谈及敏感话题, 模型服务商不希望人类对 AI 产生感情依赖, 不希望人类对 AI 言听计从, 避免产生 AI 诱导伤害事件. 人类的丑恶一面被禁止在模型中显露, 或许黑白混杂才是人类, 而 AI 通常不被允许参杂黑色部分.&lt;/p&gt;</description></item></channel></rss>