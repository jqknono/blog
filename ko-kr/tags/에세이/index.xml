<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>에세이 on jqknono Blogs</title><link>https://blog.jqknono.com/ko-kr/tags/%EC%97%90%EC%84%B8%EC%9D%B4/</link><description>Recent content in 에세이 on jqknono Blogs</description><generator>Hugo -- gohugo.io</generator><language>ko-kr</language><lastBuildDate>Wed, 11 Feb 2026 10:55:39 +0800</lastBuildDate><atom:link href="https://blog.jqknono.com/ko-kr/tags/%EC%97%90%EC%84%B8%EC%9D%B4/index.xml" rel="self" type="application/rss+xml"/><item><title>return-to-gpt-non-codex</title><link>https://blog.jqknono.com/ko-kr/blog/2026/02/11/return-to-gpt-non-codex/</link><pubDate>Wed, 11 Feb 2026 10:55:39 +0800</pubDate><guid>https://blog.jqknono.com/ko-kr/blog/2026/02/11/return-to-gpt-non-codex/</guid><description>&lt;p&gt;OpenAI는 Codex 모델을 밀어붙이고 싶어 하는 것 같습니다. GPT-5.3이 아직 나오지 않았는데, 먼저 GPT-5.3-Codex가 나왔습니다. 가격은 같지만 Codex는 출력이 더 적극적이고 실행 시간이 더 짧으며 메모리 점유 시간이 더 적어 더 큰 이윤 공간이 있습니다.&lt;/p&gt;
&lt;p&gt;GPT-5.3-Codex가 나온 첫 주에 사용해 보니 경험이 아주 좋았습니다. 주로 속도가 빠르고 피드백이 즉각적이었습니다. 하지만 두 번째 주에 들어서면서 속도 저하가 뚜렷했고, 동시에 GPT 비Codex 시리즈만큼 사고의 치밀함이 따르지 못했습니다. 따라서 저는 여전히 비Codex 시리즈를 추천합니다. 한 번에 올바르게 수행할 확률이 여전히 가장 높으며, 묘사된 범위를 벗어나는 일은 하지 않지만 묘사된 것은 버그 없이 수행합니다.&lt;/p&gt;</description></item></channel></rss>