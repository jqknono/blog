<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Essay on jqknono Blogs</title><link>https://blog.jqknono.com/categories/essay/</link><description>Recent content in Essay on jqknono Blogs</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 11 Feb 2026 10:55:39 +0800</lastBuildDate><atom:link href="https://blog.jqknono.com/categories/essay/index.xml" rel="self" type="application/rss+xml"/><item><title>return-to-gpt-non-codex</title><link>https://blog.jqknono.com/blog/2026/02/11/return-to-gpt-non-codex/</link><pubDate>Wed, 11 Feb 2026 10:55:39 +0800</pubDate><guid>https://blog.jqknono.com/blog/2026/02/11/return-to-gpt-non-codex/</guid><description>&lt;p&gt;OpenAI seems eager to push the Codex model; GPT-5.3 isn&amp;rsquo;t out yet, but GPT-5.3-Codex came out first. For the same price, Codex generates output more proactively, has shorter execution times, and occupies memory for less time, offering greater profit margins.&lt;/p&gt;
&lt;p&gt;I had a great experience using GPT-5.3-Codex during its first week of release, mainly due to its speed and timely feedback. However, by the second week, its speed decreased noticeably. Furthermore, its logical rigor is not as good as the GPT non-Codex series. Therefore, I still recommend the non-Codex series. The probability of getting it right on the first try remains the highest. It won&amp;rsquo;t do anything beyond what is described, but what is described, it does without bugs.&lt;/p&gt;</description></item></channel></rss>