<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Pensieri on jqknono Blogs</title><link>https://blog.jqknono.com/it-it/categories/pensieri/</link><description>Recent content in Pensieri on jqknono Blogs</description><generator>Hugo -- gohugo.io</generator><language>it-it</language><lastBuildDate>Wed, 11 Feb 2026 10:55:39 +0800</lastBuildDate><atom:link href="https://blog.jqknono.com/it-it/categories/pensieri/index.xml" rel="self" type="application/rss+xml"/><item><title>return-to-gpt-non-codex</title><link>https://blog.jqknono.com/it-it/blog/2026/02/11/return-to-gpt-non-codex/</link><pubDate>Wed, 11 Feb 2026 10:55:39 +0800</pubDate><guid>https://blog.jqknono.com/it-it/blog/2026/02/11/return-to-gpt-non-codex/</guid><description>&lt;p&gt;OpenAI sembra voler spingere molto i modelli Codex; GPT-5.3 non è ancora uscito, ma è stato rilasciato per primo il GPT-5.3-Codex. Allo stesso prezzo, l&amp;rsquo;output di Codex è più proattivo, i tempi di esecuzione sono più brevi e l&amp;rsquo;occupazione della memoria è inferiore, lasciando maggiore spazio al profitto.&lt;/p&gt;
&lt;p&gt;Durante la prima settimana dopo l&amp;rsquo;uscita di GPT-5.3-Codex, la mia esperienza è stata ottima, principalmente grazie alla velocità e ai feedback tempestivi. Tuttavia, entrando nella seconda settimana, il rallentamento è stato evidente; allo stesso tempo, la sua rigore logico non è pari alla serie GPT non-Codex. Pertanto, raccomando ancora la serie non-Codex: la probabilità di eseguire correttamente il compito al primo tentativo rimane la più alta. Non compirà azioni al di fuori della descrizione, ma ciò che viene descritto verrà eseguito senza bug.&lt;/p&gt;</description></item></channel></rss>