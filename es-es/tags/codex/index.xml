<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Codex on jqknono Blogs</title><link>https://blog.jqknono.com/es-es/tags/codex/</link><description>Recent content in Codex on jqknono Blogs</description><generator>Hugo -- gohugo.io</generator><language>es-es</language><managingEditor>https://blog.jqknono.com (jqknono)</managingEditor><webMaster>https://blog.jqknono.com (jqknono)</webMaster><lastBuildDate>Tue, 17 Feb 2026 10:30:00 +0800</lastBuildDate><atom:link href="https://blog.jqknono.com/es-es/tags/codex/index.xml" rel="self" type="application/rss+xml"/><item><title>Primera experiencia con GPT-5.3-Codex: de la sorpresa a la evaluación racional</title><link>https://blog.jqknono.com/es-es/blog/2026/02/17/gpt-53-codex-experience/</link><pubDate>Tue, 17 Feb 2026 10:30:00 +0800</pubDate><author>https://blog.jqknono.com (jqknono)</author><guid>https://blog.jqknono.com/es-es/blog/2026/02/17/gpt-53-codex-experience/</guid><description>&lt;p&gt;OpenAI, antes del lanzamiento de la versión oficial de GPT-5.3, lanzó primero el modelo especializado GPT-5.3-Codex. Desde una perspectiva comercial, esta decisión es comprensible. GPT-5.3-Codex tiene el mismo precio que la versión estándar de GPT-5.3, pero su salida es más enérgica, su tiempo de ejecución es más corto y consume menos memoria, lo que significa un mayor margen de beneficio. Para OpenAI, GPT-5.3-Codex es claramente una opción más rentable.&lt;/p&gt;
&lt;p&gt;Durante la primera semana después del lanzamiento de GPT-5.3-Codex, la experiencia de uso fue realmente sorprendente. La velocidad de respuesta del modelo era significativamente mejor que las versiones anteriores, y la generación de código recibía comentarios muy oportunos. Para escenarios de desarrollo que requieren iteración rápida e interacción frecuente, esta mejora de eficiencia aportó una mejora tangible de la productividad. Cuando se necesitan múltiples implementaciones en poco tiempo o validar ideas rápidamente, la característica de salida enérgica de Codex resulta especialmente útil.&lt;/p&gt;</description></item><item><title>Fórmulas de ahorro y puntos críticos en Vibe Coding</title><link>https://blog.jqknono.com/es-es/blog/2026/01/07/vibe-coding-cost-formulas/</link><pubDate>Wed, 07 Jan 2026 10:00:00 +0800</pubDate><author>https://blog.jqknono.com (jqknono)</author><guid>https://blog.jqknono.com/es-es/blog/2026/01/07/vibe-coding-cost-formulas/</guid><description>&lt;p&gt;Los modelos de facturación de las herramientas de codificación con IA se pueden clasificar en tres categorías:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Por token&lt;/strong&gt;: Incluye varias API, Claude Code (Claude Pro), Codex Cli (ChatGPT Plus), Zhipu Lite/Pro, nueva versión de Cursor, etc. En esencia, todos se facturan por token, y algunos productos ofrecen descuentos por paquetes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Por número de llamadas a la API&lt;/strong&gt;: Como OpenRouter (cuota gratuita), ModelScope, Gemini Code Assistant (1000 veces gratis al día), Chutes, etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Por número de prompts&lt;/strong&gt;: Como la versión antigua de Cursor (500 veces), Github Copilot (300 veces), etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Estos tres modelos esencialmente pagan por la inferencia del modelo y el procesamiento del contexto, con diferencias reflejadas en la granularidad de precios y la forma de los límites.&lt;/p&gt;</description></item></channel></rss>