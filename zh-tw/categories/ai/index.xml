<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on jqknono Blogs</title><link>https://blog.jqknono.com/zh-tw/categories/ai/</link><description>Recent content in AI on jqknono Blogs</description><generator>Hugo -- gohugo.io</generator><language>zh-tw</language><managingEditor>https://blog.jqknono.com (jqknono)</managingEditor><webMaster>https://blog.jqknono.com (jqknono)</webMaster><lastBuildDate>Tue, 23 Dec 2025 17:05:13 +0800</lastBuildDate><atom:link href="https://blog.jqknono.com/zh-tw/categories/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>技術部落格已死</title><link>https://blog.jqknono.com/zh-tw/blog/2025/12/23/technical-blogs-are-dead/</link><pubDate>Tue, 23 Dec 2025 17:05:13 +0800</pubDate><author>https://blog.jqknono.com (jqknono)</author><guid>https://blog.jqknono.com/zh-tw/blog/2025/12/23/technical-blogs-are-dead/</guid><description>&lt;p&gt;在過去幾年裡，人工智慧 (AI) 寫作工具如 ChatGPT、Claude 等迅速普及。它們能夠生成流暢的技術文章，甚至能模仿人類的寫作風格。這一變化引發了技術部落格圈的廣泛討論：許多人聲稱「技術部落格已死」。本文將探討 AI 工具對技術部落格的影響，並分析技術部落格的未來走向。&lt;/p&gt;
&lt;h2 id="ai-寫作工具的崛起"&gt;AI 寫作工具的崛起&lt;/h2&gt;
&lt;p&gt;AI 寫作工具的核心能力是理解自然語言並生成高品質文本。對於技術部落格作者而言，這些工具可以快速生成草稿、提供靈感，或者直接產出完整的文章。例如，當作者需要解釋一個複雜概念時，AI 可以生成清晰的說明段落；當作者缺乏時間時，AI 可以快速整理出一篇教程。&lt;/p&gt;
&lt;p&gt;然而，這種便利也帶來了副作用。大量低品質的 AI 生成內容開始充斥網際網路。這些內容往往缺乏深度，甚至包含錯誤，卻因為 SEO 優化而獲得高排名，擠壓了真正有價值的技術部落格的曝光機會。&lt;/p&gt;
&lt;h2 id="技術部落格的初衷"&gt;技術部落格的初衷&lt;/h2&gt;
&lt;p&gt;技術部落格最初是開發者分享經驗、記錄問題和建立個人品牌的方式。它的價值在於真實性和獨特性：作者將自己的實踐、思考和失敗經歷融入文章中，為讀者提供第一手的見解。&lt;/p&gt;
&lt;p&gt;AI 生成的內容雖然看似專業，但缺乏這種真實體驗。它無法分享作者在實際專案中遇到的坑，也無法提供獨特的解決方案。因此，純粹由 AI 生成的技術文章很難取代那些源自真實經驗的作品。&lt;/p&gt;
&lt;h2 id="人機協作的未來"&gt;人機協作的未來&lt;/h2&gt;
&lt;p&gt;與其將 AI 視為威脅，不如將其視為助手。聰明的技術部落客已經開始利用 AI 來提高效率：用 AI 進行頭腦風暴、檢查語法、優化表達，甚至生成程式碼範例。但文章的核心觀點和獨特洞察仍然來自作者本人。&lt;/p&gt;
&lt;p&gt;這種協作模式可以讓作者更專注於創造性工作，而將重複性任務交給 AI。最終產出的文章既保留了作者的個人色彩，又具備更高的可讀性和準確性。&lt;/p&gt;
&lt;h2 id="技術部落格的轉型"&gt;技術部落格的轉型&lt;/h2&gt;
&lt;p&gt;面對 AI 的衝擊，技術部落格需要轉型。未來的技術部落格可能會更加注重深度分析、獨家觀點和互動性。例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;深度長文&lt;/strong&gt;：深入探討某個技術領域，提供 AI 難以複製的專業見解。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;專案日誌&lt;/strong&gt;：記錄真實的開發過程，分享成功與失敗的經驗。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;影音/播客&lt;/strong&gt;：多媒體形式能更好地傳達情感和細節。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;社群互動&lt;/strong&gt;：透過評論區、論壇等方式與讀者直接交流，建構知識共同體。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;這些形式都依賴於人類的創造力和經驗，是 AI 目前無法完全替代的。&lt;/p&gt;
&lt;h2 id="結論"&gt;結論&lt;/h2&gt;
&lt;p&gt;「技術部落格已死」或許是一種誇張的說法。AI 確實改變了技術寫作的格局，但同時也為作者提供了新的工具。那些能夠適應變化、將 AI 作為輔助而非替代的部落客，不僅不會消失，反而會產出更優質的內容。技術部落格不會死，它只會以更豐富的形式繼續存在。&lt;/p&gt;</description></item><item><title>OpenAI的取名藝術鑑賞</title><link>https://blog.jqknono.com/zh-tw/blog/2025/12/12/openai%E7%9A%84%E5%8F%96%E5%90%8D%E8%97%9D%E8%A1%93%E9%91%91%E8%B3%9E/</link><pubDate>Fri, 12 Dec 2025 11:46:01 +0800</pubDate><author>https://blog.jqknono.com (jqknono)</author><guid>https://blog.jqknono.com/zh-tw/blog/2025/12/12/openai%E7%9A%84%E5%8F%96%E5%90%8D%E8%97%9D%E8%A1%93%E9%91%91%E8%B3%9E/</guid><description>&lt;p&gt;這裡&lt;a href="https://platform.openai.com/docs/models/"&gt;https://platform.openai.com/docs/models/&lt;/a&gt;記錄了 OpenAI 的所有模型。&lt;/p&gt;
&lt;p&gt;太遠了的不說，從 &lt;code&gt;GPT-4&lt;/code&gt; 系列及同世代模型聊起，這是我彙總的表格：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名稱&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;th&gt;模型卡片連結&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;ChatGPT-4o&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;GPT-4o model used in ChatGPT&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/chatgpt-4o-latest"&gt;https://platform.openai.com/docs/models/chatgpt-4o-latest&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;An older high-intelligence GPT model&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4"&gt;https://platform.openai.com/docs/models/gpt-4&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4 Turbo&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;An older high-intelligence GPT model&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4-turbo"&gt;https://platform.openai.com/docs/models/gpt-4-turbo&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4 Turbo Preview&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deprecated An older fast GPT model&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4-turbo-preview"&gt;https://platform.openai.com/docs/models/gpt-4-turbo-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4.1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Smartest non-reasoning model&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4.1"&gt;https://platform.openai.com/docs/models/gpt-4.1&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4.1 mini&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Smaller, faster version of GPT-4.1&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4.1-mini"&gt;https://platform.openai.com/docs/models/gpt-4.1-mini&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4.1 nano&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fastest, most cost-efficient version of GPT-4.1&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4.1-nano"&gt;https://platform.openai.com/docs/models/gpt-4.1-nano&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4.5 Preview&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Deprecated large model.&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4.5-preview"&gt;https://platform.openai.com/docs/models/gpt-4.5-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fast, intelligent, flexible GPT model&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o"&gt;https://platform.openai.com/docs/models/gpt-4o&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o Audio&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;GPT-4o models capable of audio inputs and outputs&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-audio-preview"&gt;https://platform.openai.com/docs/models/gpt-4o-audio-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o mini&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fast, affordable small model for focused tasks&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-mini"&gt;https://platform.openai.com/docs/models/gpt-4o-mini&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o mini Audio&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Smaller model capable of audio inputs and outputs&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-mini-audio-preview"&gt;https://platform.openai.com/docs/models/gpt-4o-mini-audio-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o mini Realtime&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Smaller realtime model for text and audio inputs and outputs&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-mini-realtime-preview"&gt;https://platform.openai.com/docs/models/gpt-4o-mini-realtime-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o mini Search Preview&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fast, affordable small model for web search&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-mini-search-preview"&gt;https://platform.openai.com/docs/models/gpt-4o-mini-search-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o mini Transcribe&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Speech-to-text model powered by GPT-4o mini&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-mini-transcribe"&gt;https://platform.openai.com/docs/models/gpt-4o-mini-transcribe&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o mini TTS&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Text-to-speech model powered by GPT-4o mini&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-mini-tts"&gt;https://platform.openai.com/docs/models/gpt-4o-mini-tts&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o Realtime&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Model capable of realtime text and audio inputs and outputs&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-realtime-preview"&gt;https://platform.openai.com/docs/models/gpt-4o-realtime-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o Search Preview&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;GPT model for web search in Chat Completions&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-search-preview"&gt;https://platform.openai.com/docs/models/gpt-4o-search-preview&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o Transcribe&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Speech-to-text model powered by GPT-4o&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-transcribe"&gt;https://platform.openai.com/docs/models/gpt-4o-transcribe&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;GPT-4o Transcribe Diarize&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Transcription model that identifies who&amp;rsquo;s speaking when&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/gpt-4o-transcribe-diarize"&gt;https://platform.openai.com/docs/models/gpt-4o-transcribe-diarize&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o1&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Previous full o-series reasoning model&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o1"&gt;https://platform.openai.com/docs/models/o1&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o1-mini&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A small model alternative to o1&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o1-mini"&gt;https://platform.openai.com/docs/models/o1-mini&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o1-pro&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Version of o1 with more compute for better responses&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o1-pro"&gt;https://platform.openai.com/docs/models/o1-pro&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o3&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Reasoning model for complex tasks, succeeded by GPT-5&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o3"&gt;https://platform.openai.com/docs/models/o3&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o3-pro&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Version of o3 with more compute for better responses&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o3-pro"&gt;https://platform.openai.com/docs/models/o3-pro&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o3-mini&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;A small model alternative to o3&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o3-mini"&gt;https://platform.openai.com/docs/models/o3-mini&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;code&gt;o4-mini&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Fast, cost-efficient reasoning model, succeeded by GPT-5 mini&lt;/td&gt;
&lt;td&gt;&lt;a href="https://platform.openai.com/docs/models/o4-mini"&gt;https://platform.openai.com/docs/models/o4-mini&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;針對音頻(Audio)、實時(Realtime)、搜尋(Search)、音頻轉文字(Transcribe)、文字轉語音(TTS) 等場景，全都有相應模型。&lt;br&gt;
對同一場景，如音頻&lt;code&gt;Audio&lt;/code&gt;場景，提供了 &lt;code&gt;GPT-4o Audio&lt;/code&gt; 和 &lt;code&gt;GPT-4o mini Audio&lt;/code&gt; 兩個模型，用戶需要透過嘗試決定能否接受效果。&lt;br&gt;
音頻轉文字&lt;code&gt;Transcribe&lt;/code&gt;場景，提供了 &lt;code&gt;GPT-4o Transcribe&lt;/code&gt;、&lt;code&gt;GPT-4o mini Transcribe&lt;/code&gt;、&lt;code&gt;GPT-4o Transcribe Diarize&lt;/code&gt; 三個模型。&lt;br&gt;
&lt;code&gt;ChatGPT-4o&lt;/code&gt; 是 &lt;code&gt;GPT-4o&lt;/code&gt; 的特別版，用於 ChatGPT 中，其他場景不能用。&lt;br&gt;
&lt;code&gt;GPT-4.1&lt;/code&gt;比&lt;code&gt;GPT-4&lt;/code&gt;更好，是符合直覺的，但&lt;code&gt;GPT-4o&lt;/code&gt;(4 歐)和&lt;code&gt;GPT-4.1&lt;/code&gt;或&lt;code&gt;GPT-4&lt;/code&gt;卻不好比較。時間線上，先出現的是&lt;code&gt;GPT-4&lt;/code&gt;，然後是&lt;code&gt;GPT-4o&lt;/code&gt;，再然後是&lt;code&gt;GPT-4.1&lt;/code&gt;，當然，仍然無法判斷&lt;code&gt;GPT-4o&lt;/code&gt;和&lt;code&gt;GPT-4.1&lt;/code&gt;誰更好。&lt;br&gt;
這裡插入下對「好」的定義，我個人將數學和推理結果正確率高作為「好」的定義，完全不將「速度」納入「好」的定義。但此前 OpenAI 可能認為速度和智慧同樣重要，因此出現奇怪的模型對比。進入 GPT-5 時代後，慶幸 OpenAI 開始放棄速度，追求智慧，至此模型間的對比才沒有歧義。一個快速的錯誤回答是浪費時間，沒有意義。&lt;/p&gt;</description></item></channel></rss>