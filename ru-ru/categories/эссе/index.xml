<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Эссе on jqknono Blogs</title><link>https://blog.jqknono.com/ru-ru/categories/%D1%8D%D1%81%D1%81%D0%B5/</link><description>Recent content in Эссе on jqknono Blogs</description><generator>Hugo -- gohugo.io</generator><language>ru-ru</language><lastBuildDate>Wed, 11 Feb 2026 10:55:39 +0800</lastBuildDate><atom:link href="https://blog.jqknono.com/ru-ru/categories/%D1%8D%D1%81%D1%81%D0%B5/index.xml" rel="self" type="application/rss+xml"/><item><title>Возврат к GPT Non-Codex</title><link>https://blog.jqknono.com/ru-ru/blog/2026/02/11/return-to-gpt-non-codex/</link><pubDate>Wed, 11 Feb 2026 10:55:39 +0800</pubDate><guid>https://blog.jqknono.com/ru-ru/blog/2026/02/11/return-to-gpt-non-codex/</guid><description>&lt;p&gt;OpenAI, похоже, очень хочет продвигать модель Codex. GPT-5.3 еще не вышла, а сначала выпустили GPT-5.3-Codex. По той же цене Codex генерирует ответы активнее, имеет меньшее время выполнения, занимает память на меньшее время и обеспечивает более высокую прибыльность.&lt;/p&gt;
&lt;p&gt;В первую неделю после выхода GPT-5.3-Codex опыт использования был отличным, в основном благодаря высокой скорости и своевременной обратной связи. Но во вторую неделю скорость заметно снизилась, а вдобавок его тщательность мышления уступает серии GPT non-Codex. Поэтому я все же рекомендую серию non-Codex: вероятность сделать все правильно с первого раза у нее по-прежнему самая высокая. Она не делает ничего сверх того, что описано, но то, что описано, выполняет без багов.&lt;/p&gt;</description></item></channel></rss>